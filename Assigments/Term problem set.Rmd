---
title: "Term problem set"
author: "Alyssa Vanderbeek (amv2187)"
date: "2 December 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(tidyverse)
library(dfcrm)
```


### Problem 1: (Preliminaries) For a randomized, placebo-controlled study of a statin for lowering LDL, we consider a reduction in LDL by 10mg/dL on average to be clinically meaningful, whereas patients in the placebo will experience no change on average. Assume that the variance of LDL is equal to 20mg/dL at both baseline and the 3-month follow-up in both groups.

### a. What additional information or assumption(s) do you need to calculate a sample size?

We need to know the following in order to compute a sample size:
* target type I error rate and power
* the average LDL level in the placebo group 

### b. State the assumptions you make and calculate the sample size required for a two-sided test at 5\% significance.

Let's suppose that \alpha=0.05 and target power is 90\%. Consider that the population LDL is approximately normally distributed.

Since the primary outcome is the difference in differences between the two groups, we also have to make assumptions about the correlation between the measurements in each group at baseline and 3 months. I used simulation to estimate a reasonable correlation between the paired measurements in each treatment group. 

```{r}
n.sim = 1e3

d.null = 0
d.alt = 10
var = 20

ldl.null = 145
ldl.alt = ldl.null - d.alt

cor_sim = function(ldl, d, var, vals = FALSE){
  p_base = rnorm(1e3, ldl, sqrt(var))
  add = rnorm(1e3, d, sqrt(var))
  p_3 = p_base - add
  
  if (vals == TRUE) {
    return(list(c = cor(p_base, p_3),
                p_base = p_base,
                p_3 = p_3))
  } else {
    return(cor(p_base, p_3))
  }
}

placebo = cor_sim(ldl = ldl.null, d = d.null, var = var, vals = TRUE)
exp = cor_sim(ldl = ldl.null, d = d.alt, var = var, vals = TRUE)

par(mfrow = c(1, 2))
plot(x = placebo$p_base, y = placebo$p_3,
     xlab = "Baseline LDL", ylab = "3 month LDL", main = "Placebo",
     xlim = c(120, 170), ylim = c(120, 170))
plot(x = exp$p_base, y = exp$p_3,
     xlab = "Baseline LDL", ylab = "3 month LDL", main = "Experimental",
     xlim = c(120, 170), ylim = c(120, 170))


cor_placebo = sapply(X = 1:n.sim, function(i){ 
  cor_sim(ldl = ldl.null, d = d.null, var = var)
})
mean(cor_placebo)
cor_exp = sapply(X = 1:n.sim, function(i){ 
  cor_sim(ldl = ldl.null, d = d.alt, var = var)
})
mean(cor_exp)
```

Based on these simulations, it's reasonable to assume that the correlation between baseline and 3-month measurements in both groups is ~0.7. Then the distributions of the differences in each treatment group are

$$
d &\sim N(E(\mu_{\text{base}} - \mu_{\text{3mo}}), Var(\mu_{\text{base}} - \mu_{\text{3mo}})) \\
&\sim N(E(\mu_{\text{base}}) + E(\mu_{\text{3mo}}), Var(\mu_{\text{base}}) + Var(\mu_{\text{3mo}}) - 2Cor(\mu_{\text{base}}, \mu_{\text{3mo}}) sd(\mu_{\text{base}}) sd(\mu_{\text{3mo}})) \\
&\sim N(0, \sigma^2 + \sigma^2 - (2)(0.7)(\sigma^2))) \\
&\sim N(0, 1.6\sigma^2)
$$

Even though are assumed to know the population variance in LDL, it may be most appropriate to conduct a t-test. This may be especially appropriate for small sample sizes (n<30 in each group), where we cannot assume the observed data are normally distributed. Again, using simulations to find the sample sizes that corresponds to a type I error of at most 0.05, and power of at least 0.9:

```{r}
alpha = 0.05
pow.target = 0.9

## Function to find sample size that meets target alpha and power
ss_calc = function(target_alpha, target_pow, null, alt, var) {
  i = 2
  t1 = 0.5
  pow = 0.5
  while (pow < target_pow || t1 > target_alpha) { # do I need to make sure that type I error is also below target?
    
    ## Under null hypothesis
    h.null = sapply(1:n.sim, function(j) {
      placebo = rnorm(i, null, sqrt(0.6*var))
      exp.null = rnorm(i, null, sqrt(0.6*var))
      
      return(t.test(exp.null, placebo, alternative = "greater", var.equal = TRUE)[[1]])
    })
    
    ## Under alternative hypothesis
    h.alt = sapply(1:n.sim, function(j) {
      placebo = rnorm(i, null, sqrt(0.6*var))
      exp.alt = rnorm(i, alt, sqrt(0.6*var))
      
      return(t.test(exp.alt, placebo, alternative = "greater", var.equal = TRUE)[[1]])
      #return((mean(exp.alt) - mean(placebo)) / sqrt(((2.6*var))/(2*i)))
    })
    
    # simulated type I error and power
    t1 = mean(h.null >= qt(1 - target_alpha, df = 2*i - 1)) # test for greater than critical value since reduction is taken as a positive value
    pow = mean(h.alt >= qt(1 - target_alpha, df = 2*i - 1))
    
    #print(c("n" = i, "type 1" = t1, "power" = pow))
    i = i + 1
  }
  
  return(i - 1)
}

# simulate 200 times
set.seed(1)
t = sapply(1:200, function(i){
  ss_calc(target_alpha = alpha, 
          target_pow = pow.target, 
          null = d.null, 
          alt = d.alt, 
          var = var)
})
mean(t) # average number of subjects 
```

Rounding up, a sample size of `r ceiling(mean(t))` in each group is appropriate.


### Problem 2: (Go/no-go) In a two-stage trial of an experimental treatment with a planned futility interim analysis, 14 patients are first enrolled and treated in the first stage. An additional of 20 patients will be enrolled and treated if there is at least 1 response in the first stage. At the end of the trial, the treatment is deemed promising ("go") when there are at least 4 responses in all enrolled patients.

### a. Suppose the true response rate is 5\%. What is the expected value of the sample size?
```{r}
p = 0.05
n1 = 14
e1 = 1
ntotal = 34
e2 = 4
n2 = ntotal - n1

# P(stopping at stage 1 | p = 0.05)
x = pbinom(e1 - 1, n1, p) # 0.48

# Expected sample size
n_expected = ceiling(n1*x + ntotal*(1 - x)) # 24.24

```

For a true response rate of 5\%, the expected sample size of the trial is 25 subjects. 

### b. Suppose the true response rate is 5\%. Evaluate the probability of a "go" decision.

```{r}
f = c()
for (i in e1:n1) {
  f[i] = dbinom(i, n1, p) * (1 - pbinom(e2 - i - 1, n2, p))
}
pgo1 = sum(f, na.rm = T)
```

The probability of a "go" decision is given by 
\begin{align*}
Pr(go) &= Pr(S_{14} \geq 1, S_{34} \geq 4 | p = 0.05) \\
&= ... = \Sigma_{n=1}^{14} [Pr(S_14 = n|p = 0.05) * \Sigma_{m = n}^{34} [Pr(S_{20} 
\geq 4-m | p = 0.05)]].
\end{align*}

For a true response rate of 5\%, the probability of a "go" decision is ~0.08.

### c. Suppose the true response rate is 20\%. Evaluate the probability of a "go" decision.
```{r}
p = 0.2
f = c()
for (i in e1:n1) {
  f[i] = dbinom(i, n1, p) * (1 - pbinom(e2 - i - 1, n2, p))
}
pgo2 = sum(f, na.rm = T)
```

Using the same formulation for calculating $Pr(go)$ as above, for a true response rate of 20\%, the probability of making a "go" decision is ~0.90.

### d. Using your result in (b) as type I error rate and (c) as power, evaluate the sample size required for a fixed design with null response 5\% and alternative response 20\%.
```{r}
p0 = 0.05
p1 = 0.2
alpha = pgo1
power = pgo2

ss = list()
for (i in 1:50) {
  n = i
  s = seq(1, i, 1)
  
  t1 = 1 - pbinom(s - 1, n, p0)
  pow = 1 - pbinom(s - 1, n, p1)
  index = intersect(which(t1 <= alpha), which(pow >= power))
  
  if (length(index) == 0) {
    ss[[i]] = NA
  } else {
    ss[[i]] = c(s[index[1]], n)
  }
}

n = ss[which(!is.na(ss))[1]][[1]][2]
s = ss[which(!is.na(ss))[1]][[1]][1]


```

Setting type I error to 0.08 and power to 0.90, we find that the sample size required for a fixed design is `r n` subjects, for which we reject the null in favor of the alternative when we see a response from at least `r s`. 


## Problem 3: (Predictive distribution) Suppose X1 follows an exponential distribution with rate .

### a. Give a conjugate prior for Derive its posterior distribution and the corresponding prior predictive distribution of X1.

### b. Suppose X1 and X2 are exchangeable. Derive the posterior predictive distribution of X2 given X1.

### c. Derive the posterior predictive distribution of (X1 + X2)/2 given X1.


## Problem 4: Suppose the toxicity probability of dose level 1 is 0.25. Calculate the probability that the 3+3 algorithm will declare dose level 1 safe (i.e., below the MTD).


```{r}
p = dbinom(0, 3, 0.25) + dbinom(1, 6,0.25)*dbinom(1, 3, 0.25)
```

With a toxicity probability of 0.25, the 3+3 algorithm will declare dose safe (i.e. at or below the MTD) with probability `r round(p, 2)`.

## Problem 5: 


```{r}
# p.true = c(0.02, 0.04, 0.10, 0.25, 0.50)
# n.dose = length(p.true)
# N = 1e3
# 
# set.seed(1)
# sims = performance_3plus3(0.025, 1, N)
# 
# mtd = unlist(lapply(sims, `[[`, 4))
# mean(mtd.1)
```


## Problem 6

```{r}
library(dfcrm)
target = 0.1
prior = c(0.05, 0.12, 0.25, 0.40, 0.55)
trueP = c(0.02, 0.04, 0.10, 0.25, 0.50)
N = 20
crmoutput = crmsim(trueP, prior, target, N, 3, model="logistic")
crmoutput$MTD
```

## Problem 7

```{r}
crm.sim = crmsim(PI = trueP, prior = prior, target = target, n = N, x0 = 3, nsim = 10)
crm.sim$MTD
```


\pagebreak

## R code

```{r, eval=FALSE}

```

